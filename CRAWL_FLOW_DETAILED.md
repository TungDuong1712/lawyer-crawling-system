# FLOW CRAWL CHI TIáº¾T

## ğŸ¯ **Tá»”NG QUAN FLOW CRAWL**

Há»‡ thá»‘ng crawling sá»­ dá»¥ng **2-step crawling process** vá»›i Celery tasks Ä‘á»ƒ xá»­ lÃ½ song song:

```
SourceConfiguration â†’ DiscoveryURL â†’ Step 1 â†’ Step 2 â†’ Lawyer Objects
```

## ğŸ“‹ **STEP 1: CRAWL BASIC LAWYER INFO**

### **1.1 Khá»Ÿi táº¡o Crawl Session**
```python
# Admin action: start_crawl_workflow
crawl_session_task.delay(source.id)
```

### **1.2 Táº¡o DiscoveryURL Objects**
```python
# crawl_session_task()
for url in session.start_urls:
    # Parse URL Ä‘á»ƒ extract thÃ´ng tin
    domain = 'lawinfo' if 'lawinfo.com' in url else 'superlawyers'
    practice_area = url_parts[4].replace('-', ' ').title()
    state = url_parts[5].replace('-', ' ').title()
    city = url_parts[6].replace('-', ' ').title()
    
    # Táº¡o DiscoveryURL
    DiscoveryURL.objects.create(
        source_config=session,
        url=url,
        domain=domain,
        practice_area=practice_area,
        state=state,
        city=city,
        status='PENDING'
    )
```

### **1.3 Crawl Basic Info (Step 1)**
```python
# crawl_basic_lawyer_info_task()
for task in tasks:
    lawyers_found = crawl_basic_lawyer_info_task(task.id)
    task.status = 'completed'
    task.lawyers_found = lawyers_found
```

### **1.4 Chi tiáº¿t Step 1 Process**
```python
# crawl_single_url_basic()
1. Fetch HTML content tá»« URL
2. Parse HTML vá»›i BeautifulSoup
3. Extract basic lawyer info:
   - company_name
   - attorney_name
   - phone, address, website
   - practice_areas
   - detail_url (cho Step 2)
4. Táº¡o Lawyer objects vá»›i is_detail_crawled=False
5. Return sá»‘ lawyers tÃ¬m Ä‘Æ°á»£c
```

## ğŸ“‹ **STEP 2: CRAWL DETAIL INFO**

### **2.1 Trigger Step 2**
```python
# Tá»± Ä‘á»™ng trigger sau Step 1 hoáº·c manual
crawl_lawyer_detail_task.delay(lawyer_id)
```

### **2.2 Crawl Detail Info**
```python
# crawl_lawyer_detail_task()
1. Get lawyer vá»›i is_detail_crawled=False
2. Fetch detail_url HTML content
3. Parse HTML vá»›i BeautifulSoup
4. Extract detailed info:
   - attorney_details
   - law_school, bar_admissions
   - education, experience
   - badges, awards
   - lead_counsel_attorneys
5. Update Lawyer object vá»›i detailed info
6. Set is_detail_crawled=True
```

## ğŸ”„ **DETAILED FLOW DIAGRAM**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CRAWL FLOW CHI TIáº¾T                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ADMIN ACTION
   â””â”€â”€ start_crawl_workflow()
       â””â”€â”€ crawl_session_task.delay(source.id)

2. SESSION INITIALIZATION
   â”œâ”€â”€ session.status = 'DISCOVERING'
   â”œâ”€â”€ Create DiscoveryURL objects from start_urls
   â”‚   â”œâ”€â”€ Parse URL: domain, practice_area, state, city
   â”‚   â”œâ”€â”€ Create DiscoveryURL vá»›i status='PENDING'
   â”‚   â””â”€â”€ Log: "Created X discovery URLs"
   â””â”€â”€ update_crawl_progress(session_id)

3. STEP 1: BASIC CRAWL
   â”œâ”€â”€ For each DiscoveryURL:
   â”‚   â”œâ”€â”€ crawl_basic_lawyer_info_task(discovery_url_id)
   â”‚   â”‚   â”œâ”€â”€ discovery_url.status = 'RUNNING'
   â”‚   â”‚   â”œâ”€â”€ crawl_single_url_basic(discovery_url)
   â”‚   â”‚   â”‚   â”œâ”€â”€ Fetch HTML content
   â”‚   â”‚   â”‚   â”œâ”€â”€ Parse vá»›i BeautifulSoup
   â”‚   â”‚   â”‚   â”œâ”€â”€ Extract basic info:
   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ company_name
   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ attorney_name
   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ phone, address, website
   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ practice_areas
   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ detail_url
   â”‚   â”‚   â”‚   â”œâ”€â”€ Create Lawyer objects
   â”‚   â”‚   â”‚   â””â”€â”€ Return lawyers_found count
   â”‚   â”‚   â”œâ”€â”€ discovery_url.lawyers_found = lawyers_found
   â”‚   â”‚   â”œâ”€â”€ discovery_url.status = 'COMPLETED'
   â”‚   â”‚   â””â”€â”€ discovery_url.completed_at = now()
   â”‚   â””â”€â”€ Update success_count/error_count
   â””â”€â”€ session.status = 'DONE'

4. STEP 2: DETAIL CRAWL (Optional)
   â”œâ”€â”€ For lawyers with is_detail_crawled=False:
   â”‚   â”œâ”€â”€ crawl_lawyer_detail_task(lawyer_id)
   â”‚   â”‚   â”œâ”€â”€ Fetch detail_url HTML content
   â”‚   â”‚   â”œâ”€â”€ Parse vá»›i BeautifulSoup
   â”‚   â”‚   â”œâ”€â”€ Extract detailed info:
   â”‚   â”‚   â”‚   â”œâ”€â”€ attorney_details
   â”‚   â”‚   â”‚   â”œâ”€â”€ law_school, bar_admissions
   â”‚   â”‚   â”‚   â”œâ”€â”€ education, experience
   â”‚   â”‚   â”‚   â”œâ”€â”€ badges, awards
   â”‚   â”‚   â”‚   â””â”€â”€ lead_counsel_attorneys
   â”‚   â”‚   â”œâ”€â”€ Update Lawyer object
   â”‚   â”‚   â””â”€â”€ Set is_detail_crawled=True
   â”‚   â””â”€â”€ Log: "Updated detailed info for lawyer"
   â””â”€â”€ Complete detail crawling

5. PROGRESS TRACKING
   â”œâ”€â”€ update_crawl_progress(session_id)
   â”‚   â”œâ”€â”€ Calculate progress_percentage
   â”‚   â”œâ”€â”€ Update last_updated
   â”‚   â””â”€â”€ Save session
   â””â”€â”€ Log: "Session completed: X success, Y errors"
```

## ğŸ¯ **CRAWL TASKS BREAKDOWN**

### **Task 1: crawl_session_task**
```python
@shared_task
def crawl_session_task(session_id):
    """
    Main crawl session coordinator
    - Create DiscoveryURL objects
    - Trigger Step 1 for each URL
    - Update session progress
    """
```

### **Task 2: crawl_basic_lawyer_info_task**
```python
@shared_task(bind=True, max_retries=3)
def crawl_basic_lawyer_info_task(self, discovery_url_id):
    """
    Step 1: Crawl basic lawyer info
    - Extract basic information
    - Save detail URLs
    - Create Lawyer objects
    """
```

### **Task 3: crawl_lawyer_detail_task**
```python
@shared_task(bind=True, max_retries=3)
def crawl_lawyer_detail_task(self, lawyer_id):
    """
    Step 2: Crawl detailed lawyer info
    - Fetch detail URLs
    - Extract comprehensive information
    - Update Lawyer objects
    """
```

### **Task 4: update_crawl_progress**
```python
@shared_task
def update_crawl_progress(session_id):
    """
    Update crawl progress metrics
    - Calculate progress percentage
    - Update success/error counts
    - Save session status
    """
```

## ğŸ“Š **DATA FLOW**

### **Input:**
```
SourceConfiguration:
â”œâ”€â”€ start_urls: ["https://www.lawinfo.com/personal-injury/arizona/chandler/"]
â”œâ”€â”€ selectors: {"list": {...}, "detail": {...}}
â””â”€â”€ configuration: delay, retries, timeout
```

### **Processing:**
```
DiscoveryURL:
â”œâ”€â”€ url: "https://www.lawinfo.com/personal-injury/arizona/chandler/"
â”œâ”€â”€ domain: "lawinfo"
â”œâ”€â”€ practice_area: "Personal Injury"
â”œâ”€â”€ state: "Arizona"
â”œâ”€â”€ city: "Chandler"
â””â”€â”€ status: "PENDING" â†’ "RUNNING" â†’ "COMPLETED"
```

### **Output:**
```
Lawyer Objects:
â”œâ”€â”€ company_name: "Smith & Associates"
â”œâ”€â”€ attorney_name: "John Smith"
â”œâ”€â”€ phone: "(555) 123-4567"
â”œâ”€â”€ address: "123 Main St, Chandler, AZ"
â”œâ”€â”€ website: "https://smithlaw.com"
â”œâ”€â”€ detail_url: "https://www.lawinfo.com/attorney/john-smith"
â”œâ”€â”€ is_detail_crawled: False
â””â”€â”€ crawl_timestamp: "2025-10-24T10:30:00Z"
```

## ğŸ”§ **ERROR HANDLING**

### **Retry Mechanism:**
```python
@shared_task(bind=True, max_retries=3, default_retry_delay=300)
def crawl_basic_lawyer_info_task(self, discovery_url_id):
    try:
        # Crawl logic
        pass
    except Exception as exc:
        # Update status to FAILED
        discovery_url.status = 'FAILED'
        discovery_url.error_message = str(exc)
        discovery_url.save()
        
        # Retry with exponential backoff
        raise self.retry(exc=exc, countdown=60 * (self.request.retries + 1))
```

### **Memory Management:**
```python
finally:
    # Memory cleanup
    try:
        if discovery_url: del discovery_url
        import gc
        collected = gc.collect()
        logger.debug(f"Memory cleanup: {collected} objects collected")
    except Exception as cleanup_error:
        logger.warning(f"Error during memory cleanup: {cleanup_error}")
```

## ğŸ“ˆ **PROGRESS TRACKING**

### **Session Level:**
```
SourceConfiguration:
â”œâ”€â”€ total_urls: 60
â”œâ”€â”€ crawled_urls: 45
â”œâ”€â”€ success_count: 40
â”œâ”€â”€ error_count: 5
â”œâ”€â”€ progress_percentage: 75.0
â””â”€â”€ last_updated: "2025-10-24T10:30:00Z"
```

### **URL Level:**
```
DiscoveryURL:
â”œâ”€â”€ status: "COMPLETED"
â”œâ”€â”€ lawyers_found: 15
â”œâ”€â”€ started_at: "2025-10-24T10:25:00Z"
â”œâ”€â”€ completed_at: "2025-10-24T10:30:00Z"
â””â”€â”€ error_message: ""
```

## ğŸš€ **PARALLEL PROCESSING**

### **Celery Configuration:**
```python
# settings.py
CELERY_BROKER_URL = 'redis://redis:6379/0'
CELERY_RESULT_BACKEND = 'redis://redis:6379/0'
CELERY_TASK_SERIALIZER = 'json'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_RESULT_SERIALIZER = 'json'
CELERY_TIMEZONE = 'UTC'
```

### **Concurrency:**
```python
# Multiple workers can process different URLs simultaneously
# Each DiscoveryURL is processed independently
# Progress is tracked at session level
```

## ğŸ“‹ **ADMIN INTERFACE**

### **SourceConfiguration Admin:**
- **Actions**: start_crawl_workflow, download_lawyers_data
- **Display**: name, status, progress_percentage, created_at
- **Filters**: status, created_at, created_by

### **DiscoveryURL Admin:**
- **Actions**: retry_failed_urls, mark_as_pending
- **Display**: source_config, domain, practice_area, city, status, lawyers_found
- **Filters**: status, domain, practice_area, state, created_at

## ğŸ¯ **Káº¾T LUáº¬N**

**Flow crawl chi tiáº¿t bao gá»“m:**

1. **Session Initialization** - Táº¡o DiscoveryURL objects
2. **Step 1: Basic Crawl** - Extract basic lawyer info
3. **Step 2: Detail Crawl** - Extract comprehensive info
4. **Progress Tracking** - Monitor tiáº¿n Ä‘á»™ real-time
5. **Error Handling** - Retry vÃ  error recovery
6. **Memory Management** - Cleanup resources
7. **Parallel Processing** - Xá»­ lÃ½ song song hiá»‡u quáº£

**â†’ Há»‡ thá»‘ng Ä‘áº£m báº£o crawling process Ä‘Æ°á»£c quáº£n lÃ½ cháº·t cháº½, hiá»‡u quáº£ vÃ  cÃ³ thá»ƒ scale!**
