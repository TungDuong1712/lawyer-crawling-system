"""
Script chÃ­nh Ä‘á»ƒ cháº¡y há»‡ thá»‘ng crawl dá»¯ liá»‡u luáº­t sÆ°
"""

import argparse
import time
from url_generator import URLGenerator
from lawyer_crawler import LawyerCrawler
from config import LAW_DOMAINS, STATES_CITIES


def main():
    """HÃ m main Ä‘á»ƒ cháº¡y há»‡ thá»‘ng crawl"""
    
    parser = argparse.ArgumentParser(description="Há»‡ thá»‘ng crawl dá»¯ liá»‡u luáº­t sÆ°")
    parser.add_argument("--domain", help="Domain cá»¥ thá»ƒ Ä‘á»ƒ crawl (lawinfo, superlawyers, avvo)")
    parser.add_argument("--state", help="Bang cá»¥ thá»ƒ Ä‘á»ƒ crawl")
    parser.add_argument("--practice-area", help="LÄ©nh vá»±c phÃ¡p lÃ½ cá»¥ thá»ƒ")
    parser.add_argument("--limit", type=int, default=10, help="Giá»›i háº¡n sá»‘ URLs Ä‘á»ƒ crawl")
    parser.add_argument("--output", default="lawyers_data.csv", help="TÃªn file output")
    parser.add_argument("--test", action="store_true", help="Cháº¡y test vá»›i 3 URLs")
    
    args = parser.parse_args()
    
    print("ğŸ›ï¸ Há»† THá»NG CRAWL Dá»® LIá»†U LUáº¬T SÆ¯")
    print("=" * 50)
    
    # Khá»Ÿi táº¡o components
    url_generator = URLGenerator()
    crawler = LawyerCrawler()
    
    # Táº¡o danh sÃ¡ch URLs
    print("ğŸ“‹ Äang táº¡o danh sÃ¡ch URLs...")
    
    if args.test:
        # Test mode - chá»‰ crawl 3 URLs
        urls = url_generator.generate_urls()[:3]
        print(f"ğŸ§ª Test mode: {len(urls)} URLs")
    else:
        # Táº¡o URLs theo tham sá»‘
        if args.domain:
            urls = url_generator.generate_urls_for_domain(args.domain)
        elif args.state:
            urls = url_generator.generate_urls_for_state(args.state)
        else:
            urls = url_generator.generate_urls()
        
        # Lá»c theo practice area náº¿u cÃ³
        if args.practice_area:
            urls = [url for url in urls if url["practice_area"] == args.practice_area]
        
        # Giá»›i háº¡n sá»‘ URLs
        urls = urls[:args.limit]
    
    print(f"ğŸ“Š Tá»•ng sá»‘ URLs: {len(urls)}")
    
    # Hiá»ƒn thá»‹ thÃ´ng tin URLs
    domains = set(url["domain"] for url in urls)
    states = set(url["state"] for url in urls)
    practice_areas = set(url["practice_area"] for url in urls)
    
    print(f"ğŸŒ Domains: {', '.join(domains)}")
    print(f"ğŸ—ºï¸  States: {', '.join(states)}")
    print(f"âš–ï¸  Practice Areas: {', '.join(practice_areas)}")
    
    # Báº¯t Ä‘áº§u crawl
    print(f"\nğŸš€ Báº¯t Ä‘áº§u crawl {len(urls)} URLs...")
    start_time = time.time()
    
    try:
        results = crawler.crawl_multiple_urls(urls)
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\nâœ… HoÃ n thÃ nh crawl trong {duration:.2f} giÃ¢y")
        print(f"ğŸ“ˆ Káº¿t quáº£: {len(results)} luáº­t sÆ°")
        
        # LÆ°u dá»¯ liá»‡u
        if results:
            crawler.save_to_csv(args.output)
            
            # Hiá»ƒn thá»‹ thá»‘ng kÃª
            stats = crawler.get_crawl_stats()
            print(f"\nğŸ“Š THá»NG KÃŠ:")
            print(f"   â€¢ Tá»•ng luáº­t sÆ°: {stats['total_lawyers']}")
            print(f"   â€¢ CÃ³ sá»‘ Ä‘iá»‡n thoáº¡i: {stats['with_phone']}")
            print(f"   â€¢ CÃ³ email: {stats['with_email']}")
            print(f"   â€¢ CÃ³ website: {stats['with_website']}")
            
            print(f"\nğŸ“ Dá»¯ liá»‡u Ä‘Ã£ lÆ°u vÃ o: {args.output}")
        else:
            print("âŒ KhÃ´ng crawl Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Dá»«ng crawl theo yÃªu cáº§u ngÆ°á»i dÃ¹ng")
        if crawler.crawled_data:
            crawler.save_to_csv(args.output)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(crawler.crawled_data)} records vÃ o {args.output}")
    
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        if crawler.crawled_data:
            crawler.save_to_csv(args.output)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u táº¡m thá»i vÃ o {args.output}")


def show_help():
    """Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n sá»­ dá»¥ng"""
    print("""
ğŸ›ï¸ Há»† THá»NG CRAWL Dá»® LIá»†U LUáº¬T SÆ¯
=====================================

CÃCH Sá»¬ Dá»¤NG:

1. Test nhanh (3 URLs):
   python main.py --test

2. Crawl táº¥t cáº£:
   python main.py --limit 50

3. Crawl domain cá»¥ thá»ƒ:
   python main.py --domain lawinfo --limit 20

4. Crawl bang cá»¥ thá»ƒ:
   python main.py --state new-mexico --limit 30

5. Crawl lÄ©nh vá»±c cá»¥ thá»ƒ:
   python main.py --practice-area car-accident --limit 15

6. Crawl vá»›i output file tÃ¹y chá»‰nh:
   python main.py --output my_lawyers.csv --limit 25

THAM Sá»:
  --domain        Domain Ä‘á»ƒ crawl (lawinfo, superlawyers, avvo)
  --state         Bang Ä‘á»ƒ crawl (new-mexico, texas, california, etc.)
  --practice-area LÄ©nh vá»±c phÃ¡p lÃ½ (car-accident, business-litigation, etc.)
  --limit         Giá»›i háº¡n sá»‘ URLs (máº·c Ä‘á»‹nh: 10)
  --output        TÃªn file output (máº·c Ä‘á»‹nh: lawyers_data.csv)
  --test          Cháº¡y test vá»›i 3 URLs

VÃ Dá»¤:
  python main.py --domain lawinfo --state new-mexico --practice-area car-accident --limit 10
    """)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 1 or "--help" in sys.argv or "-h" in sys.argv:
        show_help()
    else:
        main()
